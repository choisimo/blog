# =============================================================================
# LiteLLM Proxy Configuration - Local Development
# =============================================================================
#
# Simplified configuration for local development.
# Does NOT require GitHub Copilot/VAS authentication.
#
# Supported providers:
#   - Google Gemini (recommended - free tier available)
#   - OpenAI
#   - Anthropic Claude
#   - Ollama (local LLM)
#
# Usage:
#   curl http://localhost:4000/v1/chat/completions \
#     -H "Authorization: Bearer sk-local-dev-key" \
#     -d '{"model": "gemini-1.5-flash", "messages": [...]}'
#
# =============================================================================

# -----------------------------------------------------------------------------
# Model List
# -----------------------------------------------------------------------------
model_list:

  # ---------------------------------------------------------------------------
  # Google Gemini (Recommended for local dev - free tier available)
  # ---------------------------------------------------------------------------
  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Google Gemini 1.5 Flash (fast, cheap)"

  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Google Gemini 1.5 Pro"

  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Google Gemini 2.0 Flash Experimental"

  # ---------------------------------------------------------------------------
  # OpenAI (Optional)
  # ---------------------------------------------------------------------------
  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-4o"

  - model_name: gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-4o Mini (fast, cheap)"

  - model_name: gpt-4-turbo
    litellm_params:
      model: gpt-4-turbo
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-4 Turbo"

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-3.5 Turbo"

  # ---------------------------------------------------------------------------
  # Anthropic Claude (Optional)
  # ---------------------------------------------------------------------------
  - model_name: claude-3.5-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Anthropic Claude 3.5 Sonnet"

  - model_name: claude-3-haiku
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Anthropic Claude 3 Haiku (fast)"

  # ---------------------------------------------------------------------------
  # Local LLM via Ollama (Optional - for offline development)
  # ---------------------------------------------------------------------------
  # Run: ollama serve (on host machine)
  # Then: ollama pull llama3.2
  - model_name: local
    litellm_params:
      model: ollama/llama3.2
      api_base: http://host.docker.internal:11434
    model_info:
      description: "Local Llama 3.2 via Ollama (default local model)"

  - model_name: local/llama3
    litellm_params:
      model: ollama/llama3.2
      api_base: http://host.docker.internal:11434
    model_info:
      description: "Local Llama 3.2 via Ollama"

  - model_name: local/codellama
    litellm_params:
      model: ollama/codellama
      api_base: http://host.docker.internal:11434
    model_info:
      description: "Local CodeLlama via Ollama"

  # ---------------------------------------------------------------------------
  # Aliases for compatibility with production config
  # ---------------------------------------------------------------------------
  # Map production model names to available local models
  - model_name: gpt-4.1
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Alias: gpt-4.1 â†’ gemini-1.5-flash (local dev)"

# -----------------------------------------------------------------------------
# Router Settings
# -----------------------------------------------------------------------------
router_settings:
  routing_strategy: simple
  num_retries: 2
  timeout: 120
  
  # Fallback configuration
  fallbacks:
    - gpt-4.1: ["gemini-1.5-flash", "gpt-4o-mini"]
    - gpt-4o: ["gemini-1.5-pro", "claude-3.5-sonnet"]
    - gemini-1.5-flash: ["gpt-4o-mini", "claude-3-haiku"]

  allowed_fails: 2
  cooldown_time: 30

# -----------------------------------------------------------------------------
# LiteLLM Settings
# -----------------------------------------------------------------------------
litellm_settings:
  set_verbose: true
  drop_params: true
  request_timeout: 120
  cache: false

# -----------------------------------------------------------------------------
# General Settings
# -----------------------------------------------------------------------------
general_settings:
  # Master key for API authentication and Admin UI access
  # Admin UI: username=admin, password=<LITELLM_MASTER_KEY value>
  master_key: os.environ/LITELLM_MASTER_KEY
  # Disable database for local development
  database_url: null
  store_model_in_db: false
  disable_spend_logs: true
