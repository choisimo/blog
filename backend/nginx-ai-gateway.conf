# =============================================================================
# AI Internal Gateway - Nginx Configuration
# =============================================================================
#
# Purpose: Abstract internal AI service URLs so that application code
#          doesn't need to know actual service locations.
#
# All internal services should use:
#   http://ai-gateway:7000    (instead of http://ai-server-backend:7016)
#   http://ai-gateway:7000/v1 (OpenAI SDK compatible)
#
# Routes:
#   /v1/*     → ai-server-backend:7016/v1/* (OpenAI SDK Compatible)
#   /chat     → ai-server-backend:7016/chat (Legacy)
#   /serve/*  → ai-server-serve:7012/*      (Direct LLM access)
#   /*        → ai-server-backend:7016/*    (Default)
#
# =============================================================================

upstream ai_backend {
    server ai-server-backend:7016;
    keepalive 32;
}

upstream ai_serve {
    server ai-server-serve:7012;
    keepalive 32;
}

# Rate limiting zone (optional, uncomment to enable)
# limit_req_zone $binary_remote_addr zone=ai_limit:10m rate=10r/s;

server {
    listen 7000;
    server_name ai-gateway;

    # Logging
    access_log /var/log/nginx/ai-gateway-access.log;
    error_log /var/log/nginx/ai-gateway-error.log warn;

    # =========================================================================
    # Health Check Endpoint
    # =========================================================================
    
    location = /health {
        access_log off;
        return 200 '{"ok":true,"service":"ai-gateway","timestamp":"$time_iso8601"}';
        add_header Content-Type application/json;
    }

    # =========================================================================
    # OpenAI SDK Compatible Endpoints (Primary)
    # http://ai-gateway:7000/v1/chat/completions → ai-server-backend:7016/v1/...
    # =========================================================================
    
    location /v1/ {
        # Optional rate limiting
        # limit_req zone=ai_limit burst=20 nodelay;

        proxy_pass http://ai_backend/v1/;
        proxy_http_version 1.1;
        
        # Headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Request-ID $request_id;
        proxy_set_header Connection "";
        
        # Pass through authentication headers
        proxy_set_header Authorization $http_authorization;
        proxy_set_header X-API-Key $http_x_api_key;
        
        # Timeouts for LLM requests (can be long)
        proxy_connect_timeout 30s;
        proxy_send_timeout 180s;
        proxy_read_timeout 180s;
        
        # Disable buffering for streaming responses (SSE)
        proxy_buffering off;
        proxy_cache off;
        chunked_transfer_encoding on;
        
        # For SSE streaming
        proxy_set_header Accept-Encoding "";
    }

    # =========================================================================
    # Models Endpoint (OpenAI Compatible)
    # http://ai-gateway:7000/v1/models → ai-server-backend:7016/v1/models
    # =========================================================================
    
    location = /v1/models {
        proxy_pass http://ai_backend/v1/models;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header Authorization $http_authorization;
        proxy_set_header X-API-Key $http_x_api_key;
        
        # Cache models list for 5 minutes
        proxy_cache_valid 200 5m;
    }

    # =========================================================================
    # Legacy Chat Endpoint (Backward Compatibility)
    # http://ai-gateway:7000/chat → ai-server-backend:7016/chat
    # =========================================================================
    
    location = /chat {
        proxy_pass http://ai_backend/chat;
        proxy_http_version 1.1;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Authorization $http_authorization;
        proxy_set_header X-API-Key $http_x_api_key;
        proxy_set_header Connection "";
        
        proxy_connect_timeout 30s;
        proxy_send_timeout 180s;
        proxy_read_timeout 180s;
        
        proxy_buffering off;
    }

    # =========================================================================
    # AI Serve Direct Access (for advanced use cases)
    # http://ai-gateway:7000/serve/* → ai-server-serve:7012/*
    # =========================================================================
    
    location /serve/ {
        proxy_pass http://ai_serve/;
        proxy_http_version 1.1;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Connection "";
        
        # Longer timeouts for direct LLM access
        proxy_connect_timeout 30s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
        
        proxy_buffering off;
    }

    # =========================================================================
    # AI Backend Health Check
    # http://ai-gateway:7000/backend/health → ai-server-backend:7016/health
    # =========================================================================
    
    location = /backend/health {
        proxy_pass http://ai_backend/health;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        
        # Short timeout for health checks
        proxy_connect_timeout 5s;
        proxy_send_timeout 5s;
        proxy_read_timeout 5s;
    }

    # =========================================================================
    # AI Serve Health Check
    # http://ai-gateway:7000/serve/health → ai-server-serve:7012/
    # =========================================================================
    
    location = /serve/health {
        proxy_pass http://ai_serve/;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        
        proxy_connect_timeout 5s;
        proxy_send_timeout 5s;
        proxy_read_timeout 5s;
    }

    # =========================================================================
    # Default: Proxy to AI Backend
    # All other requests go to ai-server-backend
    # =========================================================================
    
    location / {
        proxy_pass http://ai_backend/;
        proxy_http_version 1.1;
        
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Authorization $http_authorization;
        proxy_set_header X-API-Key $http_x_api_key;
        proxy_set_header Connection "";
        
        proxy_connect_timeout 30s;
        proxy_send_timeout 120s;
        proxy_read_timeout 120s;
    }

    # =========================================================================
    # Error Pages
    # =========================================================================
    
    error_page 502 503 504 /50x.json;
    location = /50x.json {
        internal;
        return 503 '{"ok":false,"error":"AI service temporarily unavailable","code":"SERVICE_UNAVAILABLE"}';
        add_header Content-Type application/json always;
    }
}
