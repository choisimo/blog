# =============================================================================
# LiteLLM Proxy Configuration
# =============================================================================
#
# LiteLLM provides a unified OpenAI-compatible API for all LLM providers.
# All backend services call this single endpoint instead of provider-specific APIs.
#
# Architecture:
#   Backend API -> LiteLLM Proxy (port 4000) -> Any LLM Provider
#
# Benefits:
#   - Single endpoint for all providers (OpenAI-compatible)
#   - Automatic fallback between providers
#   - Load balancing and rate limiting
#   - Cost tracking and budget management
#   - Unified logging and monitoring
#
# Usage:
#   curl http://litellm:4000/v1/chat/completions \
#     -H "Authorization: Bearer sk-litellm-master-key" \
#     -d '{"model": "gpt-4", "messages": [...]}'
#
# =============================================================================

# -----------------------------------------------------------------------------
# Model List - All available models across providers
# -----------------------------------------------------------------------------
model_list:
  # ---------------------------------------------------------------------------
  # Primary Models (Default aliases that map to fastest available)
  # ---------------------------------------------------------------------------
  # These aliases are used by the backend API - they map to Gemini for speed/cost
  - model_name: gpt-4.1
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Default model - Maps to Gemini 1.5 Flash"
      
  - model_name: gpt-4o
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "High capability model - Maps to Gemini 1.5 Pro"

  # ---------------------------------------------------------------------------
  # OpenAI Direct (Optional - when you have your own API key)
  # ---------------------------------------------------------------------------
  - model_name: openai/gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-4o Direct"

  - model_name: openai/gpt-4-turbo
    litellm_params:
      model: gpt-4-turbo
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-4 Turbo Direct"

  - model_name: openai/gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-3.5 Turbo Direct"

  # ---------------------------------------------------------------------------
  # Google Gemini - Fallback Provider
  # ---------------------------------------------------------------------------
  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Google Gemini 1.5 Flash (fast, cheap)"

  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Google Gemini 1.5 Pro"

  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Google Gemini 2.0 Flash Experimental"

  # ---------------------------------------------------------------------------
  # Anthropic Claude (Optional)
  # ---------------------------------------------------------------------------
  - model_name: claude-3.5-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Anthropic Claude 3.5 Sonnet"

  - model_name: claude-3-opus
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Anthropic Claude 3 Opus"

  - model_name: claude-3-haiku
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Anthropic Claude 3 Haiku (fast)"

  # ---------------------------------------------------------------------------
  # Local LLM (Ollama) - For development/testing
  # ---------------------------------------------------------------------------
  - model_name: local/llama3
    litellm_params:
      model: ollama/llama3
      api_base: http://host.docker.internal:11434
    model_info:
      description: "Local Llama 3 via Ollama"

  - model_name: local/codellama
    litellm_params:
      model: ollama/codellama
      api_base: http://host.docker.internal:11434
    model_info:
      description: "Local CodeLlama via Ollama"

  # ---------------------------------------------------------------------------
  # Agent-Optimized Models (Function Calling Support)
  # ---------------------------------------------------------------------------
  # These models are specifically optimized for agent use with tool calling
  
  - model_name: agent/gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "GPT-4o optimized for agent/function calling"
      supports_function_calling: true
      supports_parallel_function_calling: true

  - model_name: agent/claude-3.5-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Claude 3.5 Sonnet for agent/tool use"
      supports_function_calling: true

  - model_name: agent/gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Gemini 1.5 Pro for agent/function calling"
      supports_function_calling: true

  - model_name: agent/gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY
    model_info:
      description: "Gemini 1.5 Flash for fast agent responses"
      supports_function_calling: true

# -----------------------------------------------------------------------------
# Router Settings - Fallback and Load Balancing
# -----------------------------------------------------------------------------
router_settings:
  # Routing strategy: simple, least-busy, latency-based-routing, cost-based-routing
  routing_strategy: latency-based-routing
  
  # Number of retries before failing
  num_retries: 3
  
  # Timeout for each request (seconds)
  timeout: 120
  
  # Fallback configuration
  # If gpt-4.1 fails, try gemini alternatives
  fallbacks:
    - gpt-4.1: ["gemini-1.5-pro", "claude-3-haiku"]
    - gpt-4o: ["gemini-2.0-flash", "claude-3.5-sonnet"]

  # Context window fallbacks (for long contexts)
  context_window_fallbacks:
    - gpt-4.1: ["gemini-1.5-pro"]  # Gemini has 1M context

  # Allowed failures before marking model as failed
  allowed_fails: 3
  
  # Cooldown time after failures (seconds)
  cooldown_time: 60

# -----------------------------------------------------------------------------
# LiteLLM General Settings
# -----------------------------------------------------------------------------
litellm_settings:
  # Enable/disable request/response logging
  set_verbose: false
  
  # Drop unmapped params (for compatibility)
  drop_params: true
  
  # Request timeout (seconds)
  request_timeout: 120
  
  # Enable caching (Redis required for production)
  cache: false
  # cache_params:
  #   type: redis
  #   host: redis
  #   port: 6379
  
  # Success callbacks for monitoring
  success_callback: []
  
  # Failure callbacks for alerting
  failure_callback: []
  
  # Function Calling / Tool Use Settings
  # Enable parallel function calling where supported
  supports_function_calling: true
  
  # Allow tool_choice parameter
  supports_tool_choice: true

# -----------------------------------------------------------------------------
# General Settings
# -----------------------------------------------------------------------------
general_settings:
  # Master API key for all requests
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Disable database - we don't need internal usage tracking
  # Our own D1 database handles usage logging via backend API
  database_connection_pool_limit: 0
  disable_spend_logs: true
  
  # Enable Prometheus metrics at /metrics
  # alerting:
  #   - prometheus
