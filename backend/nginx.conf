# =============================================================================
# Nginx Reverse Proxy Configuration - Single Entry Point + LiteLLM Gateway
# =============================================================================
# Architecture:
#   Cloudflare Tunnel → nginx:80 → Internal Services
#
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │                    Cloudflare Tunnel                                    │
#   │                          │                                              │
#   │                          ▼                                              │
#   │                    ┌──────────┐                                         │
#   │                    │  nginx   │ :80 (single entry point)                │
#   │                    └────┬─────┘                                         │
#   │      ┌──────────────────┼──────────────────┬───────────────┐           │
#   │      ▼                  ▼                  ▼               ▼           │
#   │ ┌─────────┐       ┌──────────┐       ┌──────────┐    ┌──────────┐     │
#   │ │   api   │       │ litellm  │       │vas-admin │    │terminal  │     │
#   │ │  :5080  │       │  :4000   │       │  :7080   │    │  :8080   │     │
#   │ └─────────┘       └────┬─────┘       └──────────┘    └──────────┘     │
#   │                        │                                               │
#   │              ┌─────────┴─────────┐                                     │
#   │              ▼                   ▼                                     │
#   │        ┌──────────┐        ┌──────────┐                               │
#   │        │ vas-core │        │ External │                               │
#   │        │  :7012   │        │ Providers│                               │
#   │        │(Copilot) │        │(OpenAI..)│                               │
#   │        └──────────┘        └──────────┘                               │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# Routes:
#   /api/*        → api:5080              (Backend API)
#   /ai/*         → litellm:4000          (LiteLLM Gateway - OpenAI compatible)
#   /vas/*        → vas-core:7012         (VAS Core - for auth only)
#   /vas-admin/*  → vas-admin:7080        (VAS Admin UI)
#   /terminal/*   → terminal-server:8080  (Terminal WebSocket)
#
# All services communicate internally via Docker bridge network.
# Only nginx is exposed to Cloudflare Tunnel.
# =============================================================================
# Architecture:
#   Cloudflare Tunnel → nginx:80 → Internal Services
#
#   ┌─────────────────────────────────────────────────────────────────────┐
#   │                    Cloudflare Tunnel                                │
#   │                          │                                          │
#   │                          ▼                                          │
#   │                    ┌──────────┐                                     │
#   │                    │  nginx   │ :80 (single entry point)            │
#   │                    └────┬─────┘                                     │
#   │         ┌───────────────┼───────────────┬───────────────┐          │
#   │         ▼               ▼               ▼               ▼          │
#   │    ┌─────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐    │
#   │    │   api   │    │ nginx-lb │    │vas-admin │    │terminal  │    │
#   │    │  :5080  │    │:7016/7012│    │  :7080   │    │  :8080   │    │
#   │    └─────────┘    └────┬─────┘    └──────────┘    └──────────┘    │
#   │                   ┌────┴────┐                                       │
#   │              ┌────┴────┬────┴────┐                                 │
#   │              ▼         ▼         ▼                                 │
#   │          proxy-1   proxy-2   proxy-3  (x3 replicas)               │
#   │              │         │         │                                 │
#   │              ▼         ▼         ▼                                 │
#   │          core-1    core-2    core-3   (x3 replicas)               │
#   └─────────────────────────────────────────────────────────────────────┘
#
# Routes:
#   /api/*        → api:5080              (Backend API)
#   /ai/*         → nginx-lb:7016         (VAS Chat API - Load Balanced)
#   /vas/*        → nginx-lb:7012         (VAS Core - Load Balanced)
#   /vas-admin/*  → vas-admin:7080        (VAS Admin UI)
#   /terminal/*   → terminal-server:8080  (Terminal WebSocket)
#
# All services communicate internally via Docker bridge network.
# Only nginx is exposed to Cloudflare Tunnel.
# =============================================================================

# -----------------------------------------------------------------------------
# Upstream Definitions (Docker internal network)
# -----------------------------------------------------------------------------
upstream api_upstream {
    server api:5080;
    keepalive 32;
}

# LiteLLM Gateway (OpenAI-compatible)
upstream litellm_upstream {
    server litellm:4000;
    keepalive 16;
}

# AI Engine - For GitHub Copilot authentication only
# Aliases: vas-core, opencode, ai-engine (all point to same service)
upstream ai_engine_upstream {
    server ai-engine:7012;
    keepalive 8;
}

upstream ai_admin_upstream {
    server ai-admin:7080;
    keepalive 8;
}

upstream terminal_upstream {
    server terminal-server:8080;
    keepalive 8;
}

# -----------------------------------------------------------------------------
# Main Server (Single Entry Point)
# -----------------------------------------------------------------------------
server {
    listen 80;
    server_name api.nodove.com ai-serve.nodove.com _;
    
    client_max_body_size 10m;
    
    # Timeouts for long-running AI requests
    proxy_connect_timeout 60s;
    proxy_send_timeout 120s;
    proxy_read_timeout 120s;

    # Common proxy headers
    proxy_http_version 1.1;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;

    # -------------------------------------------------------------------------
    # Health Check (root)
    # -------------------------------------------------------------------------
    location = / {
        add_header Content-Type text/plain;
        return 200 "nginx ok\n";
    }

    location = /health {
        add_header Content-Type application/json;
        return 200 '{"status":"ok","service":"nginx"}';
    }

    # -------------------------------------------------------------------------
    # Backend API (/api/*)
    # -------------------------------------------------------------------------
    location /api/ {
        proxy_pass http://api_upstream;
        proxy_set_header Connection "";
        proxy_buffering off;
    }

    # -------------------------------------------------------------------------
    # LiteLLM Gateway - OpenAI Compatible AI API (/ai/*)
    # -------------------------------------------------------------------------
    # All AI requests go through LiteLLM which handles:
    # - Provider routing (OpenAI, Gemini, Claude, etc.)
    # - Automatic fallback on failures
    # - Rate limiting and load balancing
    # - Cost tracking
    #
    # Examples:
    #   POST /ai/v1/chat/completions → LiteLLM → Any Provider
    #   GET  /ai/v1/models           → List available models
    #   GET  /ai/health              → LiteLLM health check
    location /ai/ {
        rewrite ^/ai/(.*)$ /$1 break;
        proxy_pass http://litellm_upstream;
        proxy_set_header Connection "";
        proxy_buffering off;
        
        # SSE support for streaming responses
        proxy_cache off;
        chunked_transfer_encoding on;
        
        # Longer timeout for AI requests
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
    }

    # -------------------------------------------------------------------------
    # AI Engine - GitHub Copilot Auth Only (/vas/*)
    # -------------------------------------------------------------------------
    # Direct access to ai-engine for authentication purposes only.
    # All AI calls should go through /ai/* (LiteLLM Gateway).
    # Note: /vas/* path kept for backward compatibility
    location /vas/ {
        rewrite ^/vas/(.*)$ /$1 break;
        proxy_pass http://ai_engine_upstream;
        proxy_set_header Connection "";
        proxy_buffering off;
    }

    # -------------------------------------------------------------------------
    # AI Admin UI (/vas-admin/*)
    # -------------------------------------------------------------------------
    location /vas-admin/ {
        rewrite ^/vas-admin/(.*)$ /$1 break;
        proxy_pass http://ai_admin_upstream;
        proxy_set_header Connection "";
    }

    # -------------------------------------------------------------------------
    # Terminal Server WebSocket (/terminal/*)
    # -------------------------------------------------------------------------
    location /terminal/ {
        rewrite ^/terminal/(.*)$ /$1 break;
        proxy_pass http://terminal_upstream;
        
        # WebSocket support
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_read_timeout 3600s;
        proxy_send_timeout 3600s;
    }

    # -------------------------------------------------------------------------
    # Catch-all: 404
    # -------------------------------------------------------------------------
    location / {
        return 404 '{"error":"Not Found","hint":"Use /api/*, /ai/*, /vas/*, /vas-admin/*, or /terminal/*"}';
        add_header Content-Type application/json;
    }
}
