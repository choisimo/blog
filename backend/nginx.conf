# =============================================================================
# Nginx Reverse Proxy Configuration - Single Entry Point + LiteLLM Gateway
# =============================================================================
# Architecture:
#   Cloudflare Tunnel → nginx:80 → Internal Services
#
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │                    Cloudflare Tunnel                                    │
#   │                          │                                              │
#   │                          ▼                                              │
#   │                    ┌──────────┐                                         │
#   │                    │  nginx   │ :80 (single entry point)                │
#   │                    └────┬─────┘                                         │
#   │      ┌──────────────────┼──────────────────┬───────────────┐           │
#   │      ▼                  ▼                  ▼               ▼           │
#   │ ┌─────────┐       ┌──────────┐       ┌──────────┐    ┌──────────┐     │
#   │ │   api   │       │ litellm  │       │terminal  │    │ etc...   │     │
#   │ │  :5080  │       │  :4000   │       │  :8080   │    │          │     │
#   │ │         │       │(AI GW)   │       │          │    │          │     │
#   │ └─────────┘       └────┬─────┘       └──────────┘    └──────────┘     │
#   │                        │                                               │
#   │              ┌─────────┴─────────┐                                     │
#   │              ▼                   ▼                                     │
#   │        ┌──────────┐        ┌──────────┐                               │
#   │        │ OpenAI   │        │ Gemini   │                               │
#   │        │ API      │        │ API      │                               │
#   │        └──────────┘        └──────────┘                               │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# Routes:
#   /api/*        → api:5080              (Backend API)
#   /ai/*         → litellm:4000          (LiteLLM Gateway - OpenAI compatible)
#   /litellm/*    → litellm:4000          (LiteLLM direct access)
#   /terminal/*   → terminal-server:8080  (Terminal WebSocket)
#
# All services communicate internally via Docker bridge network.
# Only nginx is exposed to Cloudflare Tunnel.
# =============================================================================

# -----------------------------------------------------------------------------
# Upstream Definitions (Docker internal network)
# -----------------------------------------------------------------------------
upstream api_upstream {
    server api:5080;
    keepalive 32;
}

# LiteLLM Gateway (OpenAI-compatible)
upstream litellm_upstream {
    server litellm:4000;
    keepalive 16;
}

upstream terminal_upstream {
    server terminal-server:8080;
    keepalive 8;
}

# -----------------------------------------------------------------------------
# Main Server (Single Entry Point)
# -----------------------------------------------------------------------------
server {
    listen 80;
    server_name api.nodove.com ai-serve.nodove.com _;
    
    client_max_body_size 10m;
    
    # Timeouts for long-running AI requests
    proxy_connect_timeout 60s;
    proxy_send_timeout 120s;
    proxy_read_timeout 120s;

    # Common proxy headers
    proxy_http_version 1.1;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;

    # -------------------------------------------------------------------------
    # Health Check (root)
    # -------------------------------------------------------------------------
    location = / {
        add_header Content-Type text/plain;
        return 200 "nginx ok\n";
    }

    location = /health {
        add_header Content-Type application/json;
        return 200 '{"status":"ok","service":"nginx"}';
    }

    # -------------------------------------------------------------------------
    # Backend API (/api/*)
    # -------------------------------------------------------------------------
    location /api/ {
        proxy_pass http://api_upstream;
        proxy_set_header Connection "";
        proxy_buffering off;
    }

    # -------------------------------------------------------------------------
    # LiteLLM Gateway - OpenAI Compatible AI API (/ai/*)
    # -------------------------------------------------------------------------
    # All AI requests go through LiteLLM which handles:
    # - Provider routing (OpenAI, Gemini, Claude, etc.)
    # - Automatic fallback on failures
    # - Rate limiting and load balancing
    # - Cost tracking
    #
    # Examples:
    #   POST /ai/v1/chat/completions → LiteLLM → Any Provider
    #   GET  /ai/v1/models           → List available models
    #   GET  /ai/health              → LiteLLM health check
    location /ai/ {
        rewrite ^/ai/(.*)$ /$1 break;
        proxy_pass http://litellm_upstream;
        proxy_set_header Connection "";
        proxy_buffering off;
        
        # SSE support for streaming responses
        proxy_cache off;
        chunked_transfer_encoding on;
        
        # Longer timeout for AI requests
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
    }

    # -------------------------------------------------------------------------
    # LiteLLM Direct Access (/litellm/*)
    # -------------------------------------------------------------------------
    location /litellm/ {
        rewrite ^/litellm/(.*)$ /$1 break;
        proxy_pass http://litellm_upstream;
        proxy_set_header Connection "";
        proxy_buffering off;
        
        # SSE support
        proxy_cache off;
        chunked_transfer_encoding on;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
    }

    # -------------------------------------------------------------------------
    # Terminal Server WebSocket (/terminal/*)
    # -------------------------------------------------------------------------
    location /terminal/ {
        rewrite ^/terminal/(.*)$ /$1 break;
        proxy_pass http://terminal_upstream;
        
        # WebSocket support
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_read_timeout 3600s;
        proxy_send_timeout 3600s;
    }

    # -------------------------------------------------------------------------
    # Catch-all: 404
    # -------------------------------------------------------------------------
    location / {
        return 404 '{"error":"Not Found","hint":"Use /api/*, /ai/*, /litellm/*, or /terminal/*"}';
        add_header Content-Type application/json;
    }
}
