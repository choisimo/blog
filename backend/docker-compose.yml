# =============================================================================
# Docker Compose - Full Stack with Cloudflare Workers + LiteLLM AI Gateway
# =============================================================================
#
# Architecture (Cloudflare Workers → nginx:8080 → Services):
#
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │                         Internet                                        │
#   │                            │                                            │
#   │                            ▼                                            │
#   │                   Cloudflare Workers                                    │
#   │                   (api-gateway worker)                                  │
#   │                            │                                            │
#   │                            ▼                                            │
#   │                    Server:8080 (nginx exposed)                          │
#   │                            │                                            │
#   │                            ▼                                            │
#   │  ┌──────────────────────────────────────────────────────────────────┐  │
#   │  │                    nginx:80 (Docker internal)                    │  │
#   │  │  /api/*      → api:5080                                          │  │
#   │  │  /ai/*       → litellm:4000  (LiteLLM Gateway)                   │  │
#   │  │  /terminal/* → terminal-server:8080                              │  │
#   │  └──────────────────────────────────────────────────────────────────┘  │
#   │                                                                         │
#   │  ┌─────────────────────────────────────────────────────────────────┐   │
#   │  │              LiteLLM AI Gateway (port 4000)                     │   │
#   │  │  - OpenAI-compatible API for ALL providers                      │   │
#   │  │  - Automatic fallback: gemini → openai → claude                 │   │
#   │  │  - Load balancing, rate limiting, cost tracking                 │   │
#   │  └─────────────────────────────────────────────────────────────────┘   │
#   │                                                                         │
#   │  Internal Services (Docker bridge network only):                        │
#   │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐                       │
#   │  │    api      │ │   litellm   │ │  terminal   │                       │
#   │  │   :5080     │ │   :4000     │ │   :8080     │                       │
#   │  └─────────────┘ └─────────────┘ └─────────────┘                       │
#   │  ┌─────────────┐ ┌─────────────┐                                       │
#   │  │  embedding  │ │  chromadb   │                                       │
#   │  │    :80      │ │   :8000     │                                       │
#   │  └─────────────┘ └─────────────┘                                       │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# Public URL Routes (via api.nodove.com → Cloudflare Workers):
#   https://api.nodove.com/api/*       → Backend API
#   https://api.nodove.com/ai/*        → LiteLLM Gateway (OpenAI-compatible)
#   https://api.nodove.com/terminal/*  → Terminal WebSocket
#
# Localhost ports (for debugging):
#   127.0.0.1:8080 → nginx:80            (Main entry point)
#   127.0.0.1:4000 → litellm:4000        (LiteLLM Gateway)
#   127.0.0.1:8180 → embedding-server:80 (TEI)
#   127.0.0.1:8100 → chromadb:8000       (Vector DB)
#
# Setup:
#   1. Cloudflare Workers에서 api-gateway worker 배포
#   2. Worker에 BACKEND_ORIGIN 시크릿 설정 (http://YOUR_SERVER_IP:8080)
#   3. 서버 방화벽에서 포트 8080을 Cloudflare IP만 허용
#   4. API 키 설정 (.env):
#      - LITELLM_MASTER_KEY=sk-your-master-key
#      - GOOGLE_API_KEY=... (Gemini)
#      - OPENAI_API_KEY=sk-... (optional)
#      - ANTHROPIC_API_KEY=... (optional)
#   5. docker compose up -d
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Nginx Reverse Proxy (Entry Point from Cloudflare Workers)
  # ---------------------------------------------------------------------------
  # Cloudflare Workers connect to port 8080 on the host.
  # Routes to internal services based on URL path.
  # ---------------------------------------------------------------------------
  nginx:
    image: nginx:alpine
    depends_on:
      api:
        condition: service_healthy
    ports:
      - "8080:80"  # Exposed to Cloudflare Workers
    expose:
      - "80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ---------------------------------------------------------------------------
  # Backend API Server
  # ---------------------------------------------------------------------------
  api:
    build: .
    env_file:
      - path: .env
        required: false
    environment:
      - APP_ENV=${APP_ENV:-production}
      - HOST=0.0.0.0
      - PORT=5080
      # LiteLLM Gateway (OpenAI-compatible endpoint)
      - LITELLM_BASE_URL=http://litellm:4000
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY:-sk-litellm-master-key}
      # Legacy VAS endpoints (for GitHub Copilot auth)
      - VAS_CORE_URL=http://vas-core:7012
      # Default model settings
      - AI_DEFAULT_MODEL=gpt-4.1
    expose:
      - "5080"
    networks:
      - backend
    depends_on:
      litellm:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "node -e \"const http=require('http'); http.get('http://localhost:5080/api/v1/healthz', (r)=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # ---------------------------------------------------------------------------
  # TEI Embedding Server (Internal + localhost for self-hosted runner)
  # ---------------------------------------------------------------------------
  embedding-server:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2.3
    command: --model-id sentence-transformers/all-MiniLM-L6-v2
    ports:
      - "127.0.0.1:8180:80"
    volumes:
      - tei-data:/data
    networks:
      - backend
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # ChromaDB Vector Database (Internal + localhost for self-hosted runner)
  # ---------------------------------------------------------------------------
  chromadb:
    image: chromadb/chroma:0.5.23
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
      - ANONYMIZED_TELEMETRY=FALSE
    ports:
      - "127.0.0.1:8100:8000"
    volumes:
      - chroma-data:/chroma/chroma
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------------------------------------------------------------------------
  # Terminal Server (WebSocket PTY)
  # ---------------------------------------------------------------------------
  terminal-server:
    build:
      context: ./terminal-server
    environment:
      - ORIGIN_SECRET_KEY=${ORIGIN_SECRET_KEY:-default-secret-change-me}
      - SANDBOX_IMAGE=${SANDBOX_IMAGE:-alpine:latest}
    expose:
      - "8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - backend
    restart: unless-stopped

  # ===========================================================================
  # LiteLLM AI Gateway (Unified LLM Endpoint)
  # ===========================================================================
  # Single OpenAI-compatible endpoint for ALL LLM providers.
  # No more provider-specific code in the backend!
  # ---------------------------------------------------------------------------
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    restart: unless-stopped
    ports:
      - "127.0.0.1:4000:4000"
    expose:
      - "4000"
    environment:
      # Master API key for authentication
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-litellm-master-key}
      # Provider API Keys
      VAS_API_KEY: ${VAS_API_KEY:-file:///app/shared/auto-token.jwt}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      # Logging
      LITELLM_LOG: INFO
    command:
      - --config
      - /app/config.yaml
      - --port
      - "4000"
      - --host
      - "0.0.0.0"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml:ro
      - vas-shared:/app/shared:ro
    networks:
      - backend
    depends_on:
      ai-engine:
        condition: service_healthy
      vas-bootstrap:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:4000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    security_opt:
      - no-new-privileges:true

  # ===========================================================================
  # AI Engine (GitHub Copilot Authentication)
  # ===========================================================================
  # Unified name: ai-engine (was: vas-core, opencode, opencode-serve)
  # This service handles GitHub Copilot authentication only.
  # All AI calls go through LiteLLM gateway.
  # ---------------------------------------------------------------------------
  ai-engine:
    build:
      context: ./opencode-serve
      dockerfile: ai-serve.Dockerfile
    image: ai-engine:latest
    restart: unless-stopped
    ports:
      - "127.0.0.1:7012:7012"
    expose:
      - "7012"
    environment:
      NODE_ENV: production
      OPENCODE_HOST: 0.0.0.0
      OPENCODE_PORT: 7012
    command:
      - /app/node_modules/.bin/opencode
      - serve
      - --hostname
      - 0.0.0.0
      - --port
      - "7012"
    volumes:
      - vas-data:/home/node/.local/share/opencode
      - vas-logs:/var/log/opencode
      - ./opencode-config:/home/node/.config/opencode
    networks:
      backend:
        # Aliases for backward compatibility with legacy service names
        aliases:
          - vas-core      # Legacy name
          - opencode      # Legacy name
          - ai-engine     # New unified name
    healthcheck:
      test: ["CMD-SHELL", "node -e \"const http=require('http'); http.get('http://localhost:7012/app', ()=>process.exit(0)).on('error', ()=>process.exit(1));\" "]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    security_opt:
      - no-new-privileges:true

  # ---------------------------------------------------------------------------
  # AI Bootstrap (Auto JWT Token Generation)
  # ---------------------------------------------------------------------------
  vas-bootstrap:
    image: curlimages/curl:latest
    restart: "no"
    user: "0:0"
    entrypoint: ["/bin/sh", "/app/bootstrap-token.sh"]
    environment:
      ADMIN_URL: http://ai-admin:7080
      TOKEN_FILE: /app/shared/auto-token.jwt
      MAX_RETRIES: "60"
      RETRY_INTERVAL: "2"
    volumes:
      - ./opencode-serve/bootstrap-token.sh:/app/bootstrap-token.sh:ro
      - vas-shared:/app/shared
    networks:
      - backend
    depends_on:
      ai-admin:
        condition: service_started

  # ---------------------------------------------------------------------------
  # VAS Proxy (DEPRECATED - replaced by LiteLLM)
  # ---------------------------------------------------------------------------
  # Kept for backward compatibility. LiteLLM now handles all routing.
  # Can be removed once migration is complete.
  # ---------------------------------------------------------------------------
  vas-proxy:
    image: node:20-alpine
    restart: unless-stopped
    profiles:
      - legacy  # Only starts with: docker compose --profile legacy up
    command: ["node", "/app/auto-chat-proxy.js"]
    environment:
      OPENCODE_BASE: http://ai-engine:7012
      PROXY_PORT: 7016
      DEFAULT_PROVIDER: github-copilot
      DEFAULT_MODEL: gpt-4.1
      DEFAULT_SESSION_TITLE: Auto Session
      OPENCODE_BEARER_TOKEN_FILE: /app/shared/auto-token.jwt
      TOKEN_FILE_POLL_INTERVAL: "3000"
      TOKEN_FILE_MAX_WAIT: "180000"
      MESSAGE_TIMEOUT: "120000"
    expose:
      - "7016"
    volumes:
      - ./opencode-serve/auto-chat-proxy.js:/app/auto-chat-proxy.js:ro
      - vas-shared:/app/shared:ro
    networks:
      - backend
    depends_on:
      ai-engine:
        condition: service_healthy
      vas-bootstrap:
        condition: service_completed_successfully
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD-SHELL", "node -e \"const http=require('http'); http.get('http://localhost:7016/health', (r)=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))\""]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # ---------------------------------------------------------------------------
  # AI Admin (Token Management UI)
  # ---------------------------------------------------------------------------
  ai-admin:
    build:
      context: ./opencode-serve/go-proxy-admin
      dockerfile: Dockerfile
    image: ai-admin:latest
    restart: unless-stopped
    expose:
      - "7080"
    environment:
      OPENCODE_BASE: http://ai-engine:7012
      ADMIN_JWT_SECRET: ${ADMIN_JWT_SECRET:-auto-generated-secret-change-in-production}
      ADMIN_PORT: 7080
      ADMIN_DB_PATH: /app/data/vas-admin.db
      ADMIN_EMAIL: ${ADMIN_EMAIL:-admin@example.com}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD:-}
      AUTO_BOOTSTRAP: "true"
    volumes:
      - vas-admin-data:/app/data
    networks:
      backend:
        aliases:
          - vas-admin    # Legacy name
          - ai-admin     # New unified name
    depends_on:
      ai-engine:
        condition: service_healthy
    security_opt:
      - no-new-privileges:true

# =============================================================================
# Networks
# =============================================================================
networks:
  backend:
    driver: bridge

# =============================================================================
# Volumes
# =============================================================================
volumes:
  tei-data:
    driver: local
  chroma-data:
    driver: local
  vas-data:
    driver: local
  vas-logs:
    driver: local
  vas-admin-data:
    driver: local
  vas-shared:
    driver: local
