# =============================================================================
# LiteLLM Proxy Configuration
# =============================================================================
# https://docs.litellm.ai/docs/proxy/configs
#
# LiteLLM is the unified AI gateway. All model requests route through here.
# GitHub Token provides authentication to GitHub Models.
# =============================================================================

model_list:
  # ===========================================================================
  # GitHub Models - GPT Series
  # ===========================================================================
  - model_name: gpt-4o
    litellm_params:
      model: github/gpt-4o
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.000005
      output_cost_per_token: 0.000015

  - model_name: gpt-4o-mini
    litellm_params:
      model: github/gpt-4o-mini
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.00000015
      output_cost_per_token: 0.0000006

  - model_name: gpt-4.1
    litellm_params:
      model: github/gpt-4.1
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.000002
      output_cost_per_token: 0.000008

  - model_name: gpt-4.1-mini
    litellm_params:
      model: github/gpt-4.1-mini
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.0000004
      output_cost_per_token: 0.0000016

  - model_name: gpt-4.1-nano
    litellm_params:
      model: github/gpt-4.1-nano
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.0000001
      output_cost_per_token: 0.0000004

  # ===========================================================================
  # GitHub Models - Claude Series (Anthropic via GitHub)
  # ===========================================================================
  - model_name: claude-sonnet-4
    litellm_params:
      model: github/claude-sonnet-4-20250514
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 200000
      input_cost_per_token: 0.000003
      output_cost_per_token: 0.000015

  - model_name: claude-3.5-sonnet
    litellm_params:
      model: github/claude-3.5-sonnet
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 200000
      input_cost_per_token: 0.000003
      output_cost_per_token: 0.000015

  # ===========================================================================
  # GitHub Models - Gemini Series (Google via GitHub)
  # ===========================================================================
  - model_name: gemini-2.0-flash
    litellm_params:
      model: github/gemini-2.0-flash
      api_key: os.environ/GITHUB_TOKEN
    model_info:
      max_tokens: 1048576
      input_cost_per_token: 0.000000075
      output_cost_per_token: 0.0000003

  # ===========================================================================
  # Embedding Models (OpenAI Direct - if OPENAI_API_KEY is set)
  # ===========================================================================
  - model_name: text-embedding-3-small
    litellm_params:
      model: text-embedding-3-small
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      max_tokens: 8191
      input_cost_per_token: 0.00000002

  - model_name: text-embedding-3-large
    litellm_params:
      model: text-embedding-3-large
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      max_tokens: 8191
      input_cost_per_token: 0.00000013

# =============================================================================
# Router Settings
# =============================================================================
router_settings:
  # Retry settings
  num_retries: 3
  retry_after: 5
  
  # Timeout settings (seconds)
  timeout: 120
  
  # Load balancing strategy
  routing_strategy: simple-shuffle
  
  # Fallback examples (enable if needed)
  # fallbacks:
  #   - model_name: gpt-4o
  #     fallback_models: [claude-sonnet-4, gemini-2.0-flash]

# =============================================================================
# General Settings
# =============================================================================
general_settings:
  # Master key (for proxy access)
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Logging
  set_verbose: false
  
  # Caching (Redis - enable if needed)
  cache: false
  # cache_params:
  #   type: redis
  #   host: redis
  #   port: 6379
  
  # Cost tracking (PostgreSQL - optional)
  # database_url: os.environ/DATABASE_URL

# =============================================================================
# LiteLLM Settings
# =============================================================================
litellm_settings:
  # Drop unsupported params automatically
  drop_params: true
  
  # Success/failure callbacks (optional)
  # success_callback: ["langfuse"]
  # failure_callback: ["langfuse"]

# =============================================================================
# Environment Variables Required
# =============================================================================
# GITHUB_TOKEN         - GitHub Personal Access Token for GitHub Models
# LITELLM_MASTER_KEY   - Master key for admin operations (optional)
# OPENAI_API_KEY       - Only needed for embedding models (optional)
